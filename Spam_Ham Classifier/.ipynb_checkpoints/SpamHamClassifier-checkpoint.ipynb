{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Import Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset using Pandas\n",
    "# The dataset is collected from the UCI Machine Learning Repository \n",
    "# \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/\"\n",
    "sms = pd.read_csv('SMSSpamCollection', sep='\\t', names=[\"label\", \"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      "label      5572 non-null object\n",
      "message    5572 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.1+ KB\n",
      "None\n",
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print necessary information of the dataset\n",
    "print (sms.info())\n",
    "print (sms.head())\n",
    "print (sms['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing and cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mdutp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# This part consist with removal of punctuation marks, convert to lowercase, tokeninzing, lemmatizing, stopwords removing.\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0, len(sms)):\n",
    "    dataset = re.sub('[^a-zA-Z]', ' ', sms['message'][i])\n",
    "    dataset = dataset.lower()\n",
    "    dataset = dataset.split()\n",
    "    dataset = [ps.stem(word) for word in dataset if not word in stopwords.words('english')]\n",
    "    dataset = ' '.join(dataset)\n",
    "    corpus.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# The Bag of Words model\n",
    "word_bag = []\n",
    "\n",
    "for text in corpus:\n",
    "    words = word_tokenize(text)\n",
    "    for j in words:\n",
    "        word_bag.append(j)\n",
    "        \n",
    "word_bag = nltk.FreqDist(word_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 6312\n",
      "Common words: [('u', 1228), ('call', 695), ('go', 462), ('get', 458), ('ur', 391), ('gt', 318), ('lt', 316), ('come', 305), ('ok', 293), ('day', 293), ('free', 288), ('know', 275), ('love', 266), ('like', 261), ('time', 254), ('got', 253), ('want', 248), ('good', 248), ('text', 233), ('send', 214), ('txt', 197), ('need', 190), ('p', 188), ('one', 185), ('today', 181), ('n', 177), ('take', 174), ('see', 173), ('stop', 173), ('r', 171), ('home', 167), ('think', 166), ('repli', 164), ('lor', 162), ('k', 160), ('sorri', 160), ('still', 158), ('tell', 158), ('mobil', 157), ('back', 153), ('da', 152), ('dont', 149), ('make', 148), ('phone', 142), ('week', 141), ('pleas', 141), ('say', 140), ('hi', 140), ('work', 136), ('new', 136)]\n"
     ]
    }
   ],
   "source": [
    "print('Total words: {}'.format(len(word_bag)))\n",
    "print('Common words: {}'.format(word_bag.most_common(50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 1 0 0 1 1 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# As featureset use 2000 most common words and featureset function will return those feature\n",
    "common_word_feature = list(word_bag.keys())[:2000]\n",
    "def featureset(txt):\n",
    "    words = word_tokenize(txt)\n",
    "    feature = {}\n",
    "    for word in common_word_feature:\n",
    "        feature[word] = (word in words)\n",
    "\n",
    "    return feature\n",
    "\n",
    "\n",
    "# Convert HAM and SPAM as binary value for the model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "binary = sms['label']\n",
    "encode = LabelEncoder()\n",
    "Y = encode.fit_transform(binary)\n",
    "\n",
    "print(Y[:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sms = list(zip(corpus, Y))\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(all_sms)\n",
    "\n",
    "featuresets = [(featureset(text), label) for (text, label) in all_sms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Split the dataset into training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size = 0.30, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Use different classification algorithms for the same training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Linear Accuracy: 92.04545454545455\n",
      "Naive Bayes Accuracy: 96.5909090909091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "\n",
    "classification_algorithms = [\"SVM Linear\", \"Naive Bayes\", \"Decision Tree\", \"K Nearest Neighbors\",  \"Random Forest\", \n",
    "                             \"Logistic Regression\", \"SGD Classifier\",]\n",
    "\n",
    "classifier_set = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = zip(classification_algorithms, classifier_set)\n",
    "\n",
    "for classification_algorithms, model in models:\n",
    "    final_model = SklearnClassifier(model)\n",
    "    final_model.train(training)\n",
    "    accuracy = nltk.classify.accuracy(final_model, testing)*100\n",
    "    print(\"{} Accuracy: {}\".format(classification_algorithms, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
